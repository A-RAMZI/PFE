{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PFE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92RkB-dEoZHv"
      },
      "source": [
        "<h1>1. Téléchargement du dataset<h1>\n",
        "<h3>1.1. Acces aux fichiers compréssés <h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTrTnvCIu5Na"
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGMPzE6fuX4S"
      },
      "source": [
        "<h3>1.2. Décompression des fichiers <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9GsV2hQmyId"
      },
      "source": [
        "!unzip /content/train2014.zip >train.txt\n",
        "!unzip /content/val2014.zip >val.txt\n",
        "\"\"\" les fichiers .txt aident à vérifier l'état du processus  \"\"\"\n",
        "!rm -v /content/train2014.zip\n",
        "!rm -v /content/val2014.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AAvHRjdujzl"
      },
      "source": [
        "<h3>1.3. Chargement des informations du dataset <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-R6-ctG2ThT",
        "outputId": "b1666bc8-02ad-4b56-f706-101f47bddb8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1tRaTpuhuIeDFGZZNjOjfIch232Cr2uMS' -O dataset.pickle\n",
        "import pickle\n",
        "dataset_path='/content/dataset.pickle'\n",
        "file = open(dataset_path, 'rb')\n",
        "[dict_id,dict_data]=pickle.load(file)\n",
        "file.close()\n",
        "print('train size:',len(dict_id['train']))\n",
        "print('validation size:',len(dict_id['val']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-15 20:21:06--  https://docs.google.com/uc?export=download&id=1tRaTpuhuIeDFGZZNjOjfIch232Cr2uMS\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.119.138, 172.253.119.102, 172.253.119.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.119.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tvkj7kquk8cuq4hfvr0ho5cnk5fe8g1r/1600201200000/15956116802716578091/*/1tRaTpuhuIeDFGZZNjOjfIch232Cr2uMS?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-09-15 20:21:08--  https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tvkj7kquk8cuq4hfvr0ho5cnk5fe8g1r/1600201200000/15956116802716578091/*/1tRaTpuhuIeDFGZZNjOjfIch232Cr2uMS?e=download\n",
            "Resolving doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)... 108.177.111.132, 2607:f8b0:4001:c07::84\n",
            "Connecting to doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)|108.177.111.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘dataset.pickle’\n",
            "\n",
            "dataset.pickle          [   <=>              ]  97.46M   165MB/s    in 0.6s    \n",
            "\n",
            "2020-09-15 20:21:09 (165 MB/s) - ‘dataset.pickle’ saved [102199464]\n",
            "\n",
            "train size: 82783\n",
            "validation size: 40504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgfePMTTxeZ1"
      },
      "source": [
        "<h1>2. Partie Encodage <h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enr9XD0jxhuk"
      },
      "source": [
        "<h3>2.1 téléchargement des outils TF API <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl7pf8QXPEu-"
      },
      "source": [
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tf_slim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy4YT1FwMJ8H"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDBo-gkaMQSb"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K34QQGQxMqw3"
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPTY46cxMt1i"
      },
      "source": [
        "%%bash \n",
        "cd models/research\n",
        "pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DPlxfdtMyHF"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# patch tf1 into `utils.ops`\n",
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzPfRZbZxwQb"
      },
      "source": [
        "<h3>2.2 Chargement du modèle ssd_mobilenet_v1_coco_2017_11_17 <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZySsJSsfMzC-"
      },
      "source": [
        "def load_model(model_name):\n",
        "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "  model_file = model_name + '.tar.gz'\n",
        "  model_dir = tf.keras.utils.get_file(\n",
        "    fname=model_name, \n",
        "    origin=base_url + model_file,\n",
        "    untar=True)\n",
        "\n",
        "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
        "\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecJ2VBmM2oO"
      },
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w82Sz0bNM482"
      },
      "source": [
        "model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
        "detection_model = load_model(model_name)\n",
        "print(detection_model.signatures['serving_default'].inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXbcEVOTNVu1"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj9wY_I1NffB"
      },
      "source": [
        "\n",
        "def get_features(model, image_path):\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = np.array(Image.open(image_path).convert('RGB'))\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  # Return the results of a detection.\n",
        "  return np.array([ output_dict['detection_boxes'][0:30].reshape(-1),\n",
        "      output_dict['detection_classes'][0:30],\n",
        "      output_dict['detection_scores'][0:30]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT04zGFoyL8Y"
      },
      "source": [
        "<h3>2.3 Encodage des images ( Extraction des caractéristiques ) <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LRAR90NNhj6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KosmPT5Nn_t"
      },
      "source": [
        "\n",
        "features=[]\n",
        "ids=[]\n",
        "print('extracting train features')\n",
        "i=1\n",
        "for id,instance in dict_data['train'].items():\n",
        "  #lecture d'image à partir du dossier /content/train2014\n",
        "  file_name=instance['url'].split('/')[-1]\n",
        "  image_path='/content/train2014/'+file_name\n",
        "  ids.append(id)\n",
        "  features.append(get_features(detection_model, image_path))\n",
        "  if i%500==0 :\n",
        "    print(i//500)\n",
        "  i+=1\n",
        "pickle_file = open('/content/drive/My Drive/Projet/train.pickle', 'wb')\n",
        "pickle.dump((ids,features),pickle_file)\n",
        "pickle_file.close()\n",
        "print('end')\n",
        "#22:11:20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHkdbp3bNrxb"
      },
      "source": [
        "print('extracting val features')\n",
        "features=[]\n",
        "ids=[]\n",
        "i=1\n",
        "for id,instance in dict_data['val'].items():\n",
        "  #lecture d'image à partir du dossier /content/val2014\n",
        "  file_name=instance['url'].split('/')[-1]\n",
        "  image_path='/content/val2014/'+file_name\n",
        "  if i%500==0 :\n",
        "    print(i//500)\n",
        "  i+=1\n",
        "  ids.append(id)\n",
        "  features.append(get_features(detection_model, image_path))\n",
        "pickle_file = open('/content/drive/My Drive/Projet/val.pickle', 'wb')\n",
        "pickle.dump((ids,features),pickle_file)\n",
        "pickle_file.close()\n",
        "print('end')\n",
        "#19:32:00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oine-aQ_y6Dg"
      },
      "source": [
        "<h1>3. Partie décodage<h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyEJAhhhzE4m"
      },
      "source": [
        "3.1 Extraction des caractères :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-1XoYRxNu97"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import pickle\n",
        "nltk.download('punkt')\n",
        "# max size 250\n",
        "# max min size 70\n",
        "char_list =dict()\n",
        "char_list['begin']=0\n",
        "char_list['None']=1\n",
        "i=2\n",
        "\n",
        "freq_char=nltk.FreqDist([char for id in dict_data['train'] for caption in dict_data['train'][id]['captions'] for char in caption.lower()  ])\n",
        "for char in sorted(set([char for id in dict_data['train'] for caption in dict_data['train'][id]['captions'] for char in caption.lower()  ])):\n",
        "  if(freq_char[char] >1000 ):\n",
        "    char_list[char]=i\n",
        "    i+=1\n",
        "print(len(char_list))\n",
        "print(char_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f9Ywk3dWja8"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "selected_captions=dict()\n",
        "for id in dict_data['train'] :\n",
        "  index_list =[ i for i in range(0,len(dict_data['train'][id]['captions'])) if len(dict_data['train'][id]['captions'][i])<100 ]\n",
        "  random.shuffle(index_list)\n",
        "  selected_captions[id]=dict_data['train'][id]['captions'][index_list[0]]\n",
        "pickle_file = open('/content/drive/My Drive/Projet/selectedCaptions.pickle', 'wb')\n",
        "pickle.dump((selected_captions,char_list),pickle_file)\n",
        "pickle_file.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qDA-cOVlDTA"
      },
      "source": [
        "<h3>3.2 Code d'un générateur de données <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji3PpOmwdawV"
      },
      "source": [
        "import pickle\n",
        "def toarray( nparray_list):\n",
        "  result=[]\n",
        "  for array in nparray_list :\n",
        "    result.extend(array)\n",
        "  return result\n",
        "train_features                      = pickle.load(open('/content/drive/My Drive/Projet/train.pickle','rb'))\n",
        "(selected_captions,char_list)       = pickle.load(open('/content/drive/My Drive/Projet/selectedCaptions.pickle', 'rb'))\n",
        "\n",
        "def gen_batch( batch_size,char_list,train_features,selected_captions):\n",
        "  max_sentence_len=100\n",
        "  inputs=[]\n",
        "  targets=[]\n",
        "  for row in range(0,len(train_features[0])):\n",
        "    if row%batch_size == 0 :\n",
        "      inputs=[]\n",
        "      targets=[]\n",
        "    img_features= train_features[1][row]\n",
        "    img_features= toarray(img_features)\n",
        "    id= train_features[0][row]\n",
        "    caption=selected_captions[id]\n",
        "    input=[]\n",
        "    target=[]\n",
        "    char_index=char_list['begin'] \n",
        "    for char in caption:\n",
        "      if char in char_list:\n",
        "        codif_data=[]\n",
        "        codif_data.extend( img_features )\n",
        "        codif_data.append(char_index)\n",
        "        input.append(codif_data)\n",
        "        char_index=char_list[char] \n",
        "        target.append(char_index)\n",
        "\n",
        "    while len(input)<max_sentence_len:\n",
        "      codif_data=[]\n",
        "      codif_data.extend( img_features )\n",
        "      codif_data.append(char_index)\n",
        "      input.append(codif_data)\n",
        "      char_index=char_list['None'] \n",
        "      target.append(char_index)\n",
        "\n",
        "    inputs.append(input)\n",
        "    targets.append(target)\n",
        "\n",
        "    if (row+1)%batch_size == 0 :\n",
        "      yield (np.array(inputs),np.array(targets))\n",
        "  if (len(train_features[0]))%batch_size != 0 and False:\n",
        "    yield (np.array(inputs),np.array(targets))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  dict_data                   : dictionnaire contient les données sur le dataset ,\n",
        "  batch_size                  : taille du batch in 64,128,256,512 ,\n",
        "  words_list                  : liste des mots du vocab utilisé ,\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru5mnObwzuxc"
      },
      "source": [
        "<h3>3.3 création du modèle <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g52OXOd3irNI",
        "outputId": "340db5c0-a945-47dd-b36d-6983c28c0be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout,LSTM,Dense\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "vocab_size= len(char_list)\n",
        "def toarray( nparray_list):\n",
        "  result=[]\n",
        "  for array in nparray_list :\n",
        "    result.extend(array)\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "  dict_data                   : dictionnaire contient les données sur le dataset ,\n",
        "  batch_size                  : taille du batch in 64,128,256,512 ,\n",
        "  words_list                  : liste des mots du vocab utilisé ,\n",
        "\"\"\"\n",
        "\n",
        "model = Sequential( name='RNN' )\n",
        "model.add(  tf.keras.Input(shape=(100,181), batch_size=64) )\n",
        "model.add(  LSTM(128, return_sequences=True, stateful=True) )\n",
        "model.add(  Dropout(.2)  )\n",
        "model.add(  LSTM(128, return_sequences=True, stateful=True) )\n",
        "model.add(  Dropout(.2)  )\n",
        "model.add(  Dense(vocab_size, activation=\"softmax\") )\n",
        "model.summary()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "# Loss\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "# Accuracy\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "model.compile( optimizer=optimizer, loss=loss_object , metrics=['accuracy'] )\n",
        "\n",
        "model.reset_states()\n",
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Make a prediction on all the batch\n",
        "        predictions = model(inputs)\n",
        "        # Get the error/loss on these predictions\n",
        "        loss = loss_object(targets, predictions)\n",
        "    # Compute the gradient which respect to the loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    # Change the weights of the model\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
        "    train_loss(loss)\n",
        "    train_accuracy(targets, predictions)\n",
        "\n",
        "@tf.function\n",
        "def predict(inputs):\n",
        "    # Make a prediction on all the batch\n",
        "    predictions = model(inputs)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"RNN\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (64, 100, 128)            158720    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (64, 100, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, 100, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (64, 100, 128)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, 100, 35)             4515      \n",
            "=================================================================\n",
            "Total params: 294,819\n",
            "Trainable params: 294,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epQavNIC0Wc1"
      },
      "source": [
        "<h3>3.4 Entrainement du modèle <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p46cHfTUjlCv"
      },
      "source": [
        "batch_size=64\n",
        "with tf.device(\"gpu:0\"):\n",
        "  for epoch in range(1,301):\n",
        "      \n",
        "      for batch_inputs, batch_targets in gen_batch(batch_size,char_list,train_features,selected_captions):\n",
        "          train_step(batch_inputs, batch_targets)\n",
        "      template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
        "      print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
        "      model.reset_states()\n",
        "      log_file=open('/content/drive/My Drive/Projet/states.log','a+')\n",
        "      log_file.write(\"\"+str(train_loss.result())+\"\\t,\"+str(train_accuracy.result()*100)+\"\\n\")\n",
        "      log_file.close()\n",
        "      if epoch%10==0:\n",
        "        model.save('/content/drive/My Drive/Projet/decode%d.h5'%(epoch))\n",
        "        print('  model saved')\n",
        "\n",
        "#13:52"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVputjg259V"
      },
      "source": [
        "<h1>4. Phase des tests <h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMUB3z973YFa"
      },
      "source": [
        "<h3>4.1 Selon la précision <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVjIMMW9Ayk_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_size=64\n",
        "for batch_inputs, batch_targets in gen_batch(batch_size,char_list,train_features,selected_captions):\n",
        "  var =np.argmax(model(batch_inputs) ,axis=2)\n",
        "  for line in var :\n",
        "    for char in line :\n",
        "      if char !=1:\n",
        "        print(liste[char],end='')\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jHfIMUSltqF"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def test_model( model_path):\n",
        "  model=tf.keras.models.load_model(model_path)\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  # Loss\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "  # Accuracy\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "  model.compile( optimizer=optimizer, loss=loss_object , metrics=['accuracy'] )\n",
        "  batch_size=64\n",
        "  for inputs,targets in  gen_batch(batch_size,char_list,train_features,selected_captions):\n",
        "    predictions=model(inputs)\n",
        "    train_accuracy(targets, predictions)\n",
        "    loss = loss_object(targets, predictions)\n",
        "    train_loss(loss)\n",
        "  template = '\\r model {}, Train Loss: {}, Train Accuracy: {}'\n",
        "  print(template.format(model_path, train_loss.result(), train_accuracy.result()*100), end=\"\\n\")\n",
        "\n",
        "for num in range(10,290,10): \n",
        "  test_model('/content/drive/My Drive/Projet/decode%d.h5'%(num))\n",
        "\n",
        "\n",
        "\n",
        "#14.22.00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6jvR-S0tNxb",
        "outputId": "a5b4f9a2-e7ef-46d3-b5a6-82c03811b957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np \n",
        "import pickle\n",
        "import random\n",
        "def to_array( nparray_list):\n",
        "  result=[]\n",
        "  for array in nparray_list :\n",
        "    result.extend(array)\n",
        "  return result\n",
        "val_features                      = pickle.load(open('/content/drive/My Drive/Projet/val.pickle','rb'))\n",
        "(selected_captions,char_list)       = pickle.load(open('/content/drive/My Drive/Projet/selectedCaptions.pickle', 'rb'))\n",
        "\n",
        "def gen_val( batch_size,char_list,val_features,dict_data=dict_data):\n",
        "  max_sentence_len=100\n",
        "  inputs=[]\n",
        "  targets=[]\n",
        "  for row in range(0,len(val_features[0])):\n",
        "    if row%batch_size == 0 :\n",
        "      inputs=[]\n",
        "      targets=[]\n",
        "    img_features= val_features[1][row]\n",
        "    img_features= to_array(img_features)\n",
        "    id= val_features[0][row]\n",
        "    captions=dict_data['val'][id]['captions']\n",
        "    captions = [caption for caption in captions if len(caption)<max_sentence_len ]\n",
        "    caption = random.choice(captions)\n",
        "    input=[]\n",
        "    target=[]\n",
        "    char_index=char_list['begin'] \n",
        "    for char in caption.lower():\n",
        "      if char in char_list:\n",
        "        codif_data=[]\n",
        "        codif_data.extend( img_features )\n",
        "        codif_data.append(char_index)\n",
        "        input.append(codif_data)\n",
        "        char_index=char_list[char] \n",
        "        target.append(char_index)\n",
        "\n",
        "    while len(input)<max_sentence_len:\n",
        "      codif_data=[]\n",
        "      codif_data.extend( img_features )\n",
        "      codif_data.append(char_index)\n",
        "      input.append(codif_data)\n",
        "      char_index=char_list['None'] \n",
        "      target.append(char_index)\n",
        "\n",
        "    inputs.append(input)\n",
        "    targets.append(target)\n",
        "\n",
        "    if (row+1)%batch_size == 0 :\n",
        "      yield (np.array(inputs),np.array(targets))\n",
        "  if (len(val_features[0]))%batch_size != 0 and False:\n",
        "    yield (np.array(inputs),np.array(targets))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  dict_data                   : dictionnaire contient les données sur le dataset ,\n",
        "  batch_size                  : taille du batch in 64,128,256,512 ,\n",
        "  words_list                  : liste des mots du vocab utilisé ,\n",
        "\"\"\"\n",
        "batch_size=64\n",
        "i=0\n",
        "for inputs,targets in gen_val( batch_size,char_list,val_features):\n",
        "  print(i)\n",
        "  i+=1\n",
        "  break \n",
        "#16:23:50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMTD9Vsv4F2W"
      },
      "source": [
        "<h3>4.2 Selon la F-mesure moyenne <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePoDUZetdKrX"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def test_f_mesure(model_path):\n",
        "  model=tf.keras.models.load_model(model_path)\n",
        "  all_targets=[]\n",
        "  all_outputs=[]\n",
        "  for inputs,targets in gen_val( batch_size,char_list,val_features):\n",
        "      all_targets.extend(targets.reshape(-1))\n",
        "      outputs=model(inputs)\n",
        "      outputs=np.argmax(outputs, axis=-1).reshape(-1)\n",
        "      all_outputs.extend(outputs)\n",
        "  template = '\\r model {}, weigheted: {}, macro: {}'\n",
        "  labels=[i for i in range(0,len(char_list)) if not i==1]\n",
        "  print(template.format(model_path,f1_score(all_targets, all_outputs,labels=labels, average='weighted'),f1_score(all_targets, all_outputs,labels=labels, average='macro')), end=\"\\n\")\n",
        "#12:20:50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkiDahTB8RPN"
      },
      "source": [
        "for num in range(10,290,10): \n",
        "  test_f_mesure('/content/drive/My Drive/Projet/decode%d.h5'%(num))\n",
        "#15:43"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie6AKQS84Tum"
      },
      "source": [
        "<h3>4.1 Selon la mesure Bleu-N <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guspcFL69BdV"
      },
      "source": [
        "import nltk\n",
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import pickle\n",
        "\n",
        "def to_array( nparray_list):\n",
        "  result=[]\n",
        "  for array in nparray_list :\n",
        "    result.extend(array)\n",
        "  return result\n",
        "model_path='/content/drive/My Drive/Projet/decode280.h5'\n",
        "model=tf.keras.models.load_model(model_path)\n",
        "val_features                      = pickle.load(open('/content/drive/My Drive/Projet/val.pickle','rb'))\n",
        "(_,char_list)       = pickle.load(open('/content/drive/My Drive/Projet/selectedCaptions.pickle', 'rb'))\n",
        "\n",
        "def predict_sent( model, features,char_list):\n",
        "  sent=\"\"\n",
        "  last_output=char_list['begin'] \n",
        "  keys_list=[key for key in char_list]\n",
        "  i=0\n",
        "  input=[]\n",
        "  features=to_array(features)\n",
        "  input.extend(features)\n",
        "  input.append(last_output)\n",
        "  model.reset_states()\n",
        "  data=np.zeros((64,100,181))\n",
        "  data[0,i]=np.array(input)\n",
        "  output=model(data)\n",
        "  last_output=np.argmax(output[0,i])\n",
        "  while last_output != char_list['None'] and i<99:\n",
        "    sent+=keys_list[last_output]\n",
        "    input=[]\n",
        "    input.extend(features)\n",
        "    input.append(last_output)\n",
        "    i+=1\n",
        "    data[0,i]=np.array(input)\n",
        "    output=model(data)\n",
        "    last_output=np.argmax(output[0,i])\n",
        "  return sent\n",
        "  \n",
        "def predict_batch( model, features_batch ,char_list):\n",
        "  model.reset_states()\n",
        "  sent=[]\n",
        "  last_output=[]\n",
        "  for i in range(0,len( features_batch)):\n",
        "    features_batch[i]=to_array( features_batch[i])\n",
        "  for i in range(0,64):\n",
        "    last_output.append(char_list['begin'] )\n",
        "  keys_list=[key for key in char_list]\n",
        "  j=0\n",
        "  data=np.zeros((64,100,181))\n",
        "  input=[]\n",
        "  output=[]\n",
        "  for i in range(0,len( features_batch)):\n",
        "    input.append([])\n",
        "    input[i].extend(features_batch[i])\n",
        "    input[i].append(last_output[i])\n",
        "  while j<100:\n",
        "    model.reset_states()\n",
        "    for i in range(0,len( features_batch)):\n",
        "      data[i,j]=np.array(input[i])\n",
        "    output=model(data)\n",
        "    last_output=[]\n",
        "    for i in range(0,len( features_batch)):\n",
        "      last_output.append(np.argmax(output[i,j]))\n",
        "    input=[]\n",
        "    for i in range(0,len( features_batch)):\n",
        "      input.append([])\n",
        "      input[i].extend(features_batch[i])\n",
        "      input[i].append(last_output[i])\n",
        "    j+=1\n",
        "  result=[]\n",
        "  output=np.argmax(output,axis=2)\n",
        "  for i in range(output.shape[0]):\n",
        "    sent=\"\"\n",
        "    for j in range(output.shape[1]):\n",
        "      c=keys_list[output[i,j]]\n",
        "      if c=='None' :\n",
        "        break\n",
        "      sent+=c\n",
        "    result.append(sent)\n",
        "  return result\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CM3hkyLR-Qc"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "nltk.download('punkt')\n",
        "def predict_all_captions(model,val_features,char_list):\n",
        "  all_captions=[]\n",
        "  i=0\n",
        "  for i in range(64,len(val_features),64):\n",
        "    all_captions.extend(predict_batch( model, val_features[i-64:i] ,char_list))\n",
        "    print(i)\n",
        "  i=len(val_features)-len(val_features)%64\n",
        "  while i<len(val_features):\n",
        "    sent=predict_sent( model, val_features[i],char_list)\n",
        "    all_captions.append(sent)\n",
        "    print(i)\n",
        "    i+=1\n",
        "  print(len(val_features))\n",
        "  return all_captions\n",
        "all_captions=predict_all_captions(model, val_features[1],char_list)\n",
        "pickle_file = open('/content/drive/My Drive/Projet/all_captions.pickle', 'wb')\n",
        "pickle.dump(all_captions,pickle_file)\n",
        "pickle_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNgs_Pj81D_",
        "outputId": "db25b9e7-3c82-40fb-8846-9771504d7b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "nltk.download('punkt')\n",
        "predicted_captions  = pickle.load(open('/content/drive/My Drive/Projet/all_captions.pickle','rb'))\n",
        "\n",
        "def bleu_1(predicted,references):\n",
        "  res=[]\n",
        "  for reference in references :\n",
        "    freq_ref=FreqDist(word_tokenize( reference ))\n",
        "    freq_pred=FreqDist(word_tokenize( predicted ))\n",
        "    count_ref=sum(freq_ref.values())\n",
        "    count_pred=sum([ min(freq_pred[key],freq_ref[key]) for key in freq_ref.keys() ])\n",
        "    res.append( count_pred/ count_ref)\n",
        "  return max(res)\n",
        "\n",
        "def bleu_n(predicted,references,n):\n",
        "  res=[]\n",
        "  for reference in references :\n",
        "    liste_ref  =[ word for word  in  nltk.ngrams(word_tokenize(reference),n)]\n",
        "    liste_pred =[ word for word  in  nltk.ngrams(word_tokenize(predicted),n)]\n",
        "    freq_ref=FreqDist( liste_ref )\n",
        "    freq_pred=FreqDist( liste_pred )\n",
        "    count_ref=sum(freq_ref.values())\n",
        "    count_pred=sum([ min(freq_pred[key],freq_ref[key]) for key in freq_ref.keys() ])\n",
        "    res.append( count_pred/ count_ref)\n",
        "  return max(res)\n",
        "\n",
        "def calc_metriques( predicted_captions, val_features ,dict_data ):\n",
        "  count=len(val_features[1])\n",
        "  b1=0\n",
        "  b2=0\n",
        "  b3=0\n",
        "  b4=0\n",
        "  for i in range(0,count):\n",
        "    id=val_features[0][i]\n",
        "    predicted   = predicted_captions[i]\n",
        "    references  =  dict_data['val'][id]['captions'] \n",
        "    b1+=bleu_1(predicted,references)\n",
        "    b2+=bleu_n(predicted,references,2)\n",
        "    b3+=bleu_n(predicted,references,3)\n",
        "    b4+=bleu_n(predicted,references,4)\n",
        "  return  (b1/count,b2/count,b3/count,b4/count)\n",
        "\n",
        "print(calc_metriques( predicted_captions, val_features ,dict_data ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0.3602033744470266, 0.1214666905397255, 0.03479153426583626, 0.010338026451352279)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6EUY9Mdw6Vf"
      },
      "source": [
        "<h3>5. Informations sur le Hardware <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeqDJ7Tl3Va-",
        "outputId": "8c480ad6-c2a6-4f26-c7a7-4860f8d2f180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "!cat /etc/*release"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=18.04\n",
            "DISTRIB_CODENAME=bionic\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 18.04.5 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.5 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.5 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}